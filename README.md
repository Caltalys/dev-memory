# ğŸ§  DevMemory Pro

Personal Knowledge Base vá»›i RAG cá»¥c bá»™ â€” há»i vá» kiáº¿n trÃºc, lá»—i code, bÃ i há»c kinh nghiá»‡m tá»« Markdown notes cÃ¡ nhÃ¢n. KhÃ´ng cáº§n cloud, cháº¡y hoÃ n toÃ n trÃªn mÃ¡y local.

## TÃ­nh nÄƒng

- **Hybrid Search** â€” Vector (ChromaDB + BGE-M3) + BM25 vá»›i Reciprocal Rank Fusion
- **Date-Aware Retrieval** â€” Tá»± Ä‘á»™ng lá»c theo ngÃ y khi há»i "hÃ´m nay", "2 thÃ¡ng trÆ°á»›c", `20/10/2025`, v.v.
- **Embedding Ä‘a ngá»¯** â€” `BAAI/bge-m3` há»— trá»£ tiáº¿ng Viá»‡t tá»‘t
- **Smart Chunking** â€” Chia chunk tÃ´n trá»ng cáº¥u trÃºc Markdown vÃ  code block
- **Chat Memory** â€” SQLite lÆ°u lá»‹ch sá»­, há»i follow-up Ä‘Æ°á»£c
- **Auto Re-index** â€” Watchdog tá»± Ä‘á»™ng index khi note thay Ä‘á»•i
- **UI Dark Mode** â€” Markdown render, nguá»“n trÃ­ch dáº«n, health indicator (font Space Grotesk)
- **Docker Ready** â€” Cháº¡y 1 lá»‡nh
- **Agent Workflow** â€” `/create-dev-note` Ä‘á»ƒ táº¡o note kinh nghiá»‡m theo template tá»± Ä‘á»™ng

## Quick Start

### YÃªu cáº§u há»‡ thá»‘ng

| ThÃ nh pháº§n | YÃªu cáº§u tá»‘i thiá»ƒu |
|---|---|
| **Python** | 3.10+ |
| **RAM** | 4GB+ (khuyÃªn dÃ¹ng 8GB+) |
| **Disk** | 5GB+ (model embedding ~2GB + model LLM ~2GB) |
| **Internet** | Cáº§n khi táº£i model láº§n Ä‘áº§u |
| **Ollama** | Báº¥t ká»³ version nÃ o |

### CÃ i Ä‘áº·t Python (Ubuntu/Debian)

> Bá» qua náº¿u Ä‘Ã£ cÃ³ Python 3.10+ (`python3 --version`).

```bash
# CÃ i Python vÃ  cÃ´ng cá»¥ venv
sudo apt update
sudo apt install python3 python3.12-venv python3-pip -y

# Kiá»ƒm tra phiÃªn báº£n
python3 --version  # Python 3.12.x
```

> **LÆ°u Ã½:** TrÃªn Ubuntu/Debian, luÃ´n dÃ¹ng `python3` thay vÃ¬ `python`.

### CÃ i Ä‘áº·t Ollama vÃ  chá»n LLM

```bash
# CÃ i Ollama (náº¿u chÆ°a cÃ³)
curl -fsSL https://ollama.com/install.sh | sh

# Khá»Ÿi Ä‘á»™ng Ollama service
ollama serve &

# Chá»n model LLM phÃ¹ há»£p vá»›i RAM mÃ¡y:
ollama pull qwen2.5:1.5b  # ğŸŸ¢ RAM tháº¥p (~1.5GB) â€” KhuyÃªn dÃ¹ng cho mÃ¡y <8GB RAM
ollama pull qwen2.5:3b    # ğŸŸ¡ CÃ¢n báº±ng (~2.5GB) â€” Cáº§n 8GB+ RAM
ollama pull qwen2.5:7b    # ğŸ”´ Cháº¥t lÆ°á»£ng cao (~5GB) â€” Cáº§n GPU hoáº·c 16GB+ RAM
```

### CÃ¡ch 1: Python Local (KhuyÃªn dÃ¹ng khi dev)

```bash
# 1. Clone hoáº·c cd vÃ o thÆ° má»¥c project
cd /path/to/dev-memory

# 2. Táº¡o vÃ  kÃ­ch hoáº¡t virtual environment
python3 -m venv venv
source venv/bin/activate          # Linux/macOS
# venv\Scripts\activate.bat       # Windows

# 3. CÃ i dependencies (CPU-only â€” khÃ´ng cáº§n GPU/CUDA)
pip install -r requirements.txt
# â³ Láº§n Ä‘áº§u máº¥t 3-5 phÃºt (táº£i PyTorch CPU, ChromaDB...)

# 4. Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng
cp .env.example .env
```

Sá»­a file `.env` náº¿u cáº§n â€” Ä‘áº·c biá»‡t 2 dÃ²ng nÃ y:

```ini
# Äá»•i model LLM náº¿u cáº§n (xem pháº§n Chá»n LLM á»Ÿ trÃªn)
LLM_MODEL=qwen2.5:1.5b

# Ollama URL: localhost náº¿u cháº¡y native, host.docker.internal náº¿u trong Docker
LLM_BASE_URL=http://localhost:11434
```

```bash
# 5. Äáº·t notes vÃ o data/notes/ (xem template)
# ÄÃ£ cÃ³ sáºµn: data/notes/template.md

# 6. Cháº¡y server
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
# â³ Láº§n Ä‘áº§u: táº£i model BAAI/bge-m3 (~2GB) â€” máº¥t 5-15 phÃºt
# âœ… Tá»« láº§n 2 trá»Ÿ Ä‘i: khá»Ÿi Ä‘á»™ng trong ~30 giÃ¢y
```

Má»Ÿ trÃ¬nh duyá»‡t: **http://localhost:8000**

### CÃ¡ch 2: Docker

```bash
# 1. Build vÃ  cháº¡y
docker compose up -d

# Xem log
docker compose logs -f
```

> **LÆ°u Ã½:** Äáº£m báº£o Ollama Ä‘ang cháº¡y vÃ  `LLM_BASE_URL` trong `.env` trá» Ä‘Ãºng Ä‘áº¿n Ollama.

## Cáº¥u trÃºc Project

```
dev-memory/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py       # Settings tá»« .env
â”‚   â”œâ”€â”€ indexer.py      # Smart chunking + ChromaDB indexing
â”‚   â”œâ”€â”€ retriever.py    # Hybrid Search + Date-Aware Filter
â”‚   â”œâ”€â”€ llm.py          # Ollama client + Retry + Optimized Prompt
â”‚   â”œâ”€â”€ memory.py       # Chat history SQLite
â”‚   â”œâ”€â”€ watcher.py      # Auto re-index khi note thay Ä‘á»•i
â”‚   â””â”€â”€ main.py         # FastAPI server
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ notes/          # ğŸ“ Äáº¶T MARKDOWN NOTES VÃ€O ÄÃ‚Y
â”‚   â”‚   â””â”€â”€ template.md
â”‚   â”œâ”€â”€ chroma_db/      # Vector index (auto-generated)
â”‚   â””â”€â”€ dev_memory.db   # Chat history (auto-generated)
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ index.html      # Web UI (Space Grotesk)
â”œâ”€â”€ .agents/
â”‚   â”œâ”€â”€ rules.md        # Quy táº¯c AI agent cho project
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ create-dev-note.md  # Workflow táº¡o note kinh nghiá»‡m
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

## API Endpoints

| Endpoint | Method | MÃ´ táº£ |
|---|---|---|
| `/` | GET | Web UI |
| `/ask` | POST | Há»i RAG |
| `/ask/stream` | POST | Há»i RAG â€” Streaming |
| `/health` | GET | Tráº¡ng thÃ¡i há»‡ thá»‘ng |
| `/reindex` | POST | Trigger re-index thá»§ cÃ´ng |
| `/history/{session_id}` | GET | Lá»‹ch sá»­ chat |
| `/session/{session_id}` | DELETE | XÃ³a session |
| `/stats` | GET | Thá»‘ng kÃª |

### VÃ­ dá»¥ gá»i API

```bash
# Há»i cÃ¢u há»i
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "CÃ¡ch xá»­ lÃ½ lá»—i N+1 trong SQLAlchemy?"}'

# Health check
curl http://localhost:8000/health

# Re-index táº¥t cáº£ notes
curl -X POST http://localhost:8000/reindex \
  -H "Content-Type: application/json" \
  -d '{"full_reindex": true}'
```

## Format Note (.md)

DÃ¹ng template `data/notes/template.md` Ä‘á»ƒ táº¡o note chuáº©n:

```markdown
---
tags: [python, fastapi]
project: my-project
date: 2026-02-21
---

# TiÃªu Ä‘á» Note
...
```

## LÆ°u Ã½ Quan Trá»ng

1. **Láº§n Ä‘áº§u cháº¡y** â€” Model `BAAI/bge-m3` (~2GB) sáº½ Ä‘Æ°á»£c táº£i tá»± Ä‘á»™ng. Cáº§n Internet vÃ  kiÃªn nháº«n.
2. **Ollama trong Docker** â€” Náº¿u Ollama cháº¡y trÃªn host, Ä‘á»•i `LLM_BASE_URL=http://host.docker.internal:11434` trong `.env`.
3. **Backup data** â€” ThÆ° má»¥c `data/notes/` lÃ  toÃ n bá»™ tri thá»©c cá»§a báº¡n. Push lÃªn Git private repo hoáº·c sync lÃªn Cloud Drive thÆ°á»ng xuyÃªn.
4. **ThÃ³i quen** â€” Tool chá»‰ hiá»‡u quáº£ khi báº¡n duy trÃ¬ viáº¿t note. Äáº·t reminder cuá»‘i tuáº§n Ä‘á»ƒ review.

## Kháº¯c Phá»¥c Sá»± Cá»‘

| Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch sá»­a |
|---|---|---|
| `python not found` | Ubuntu dÃ¹ng `python3` | DÃ¹ng `python3` thay vÃ¬ `python` |
| `ensurepip not available` | Thiáº¿u gÃ³i venv | `sudo apt install python3.12-venv` |
| `externally-managed-environment` | KhÃ´ng dÃ¹ng venv | Táº¡o venv trÆ°á»›c: `python3 -m venv venv` |
| `np.float_ removed` | NumPy 2.x khÃ´ng tÆ°Æ¡ng thÃ­ch | `pip install "numpy>=1.24.0,<2.0"` |
| `bm25s==0.1.8 not found` | Version bá»‹ yanked | ÄÃ£ fix trong `requirements.txt` (dÃ¹ng `0.1.10`) |
| `AssertionError` khi `pip install` | pip version cÅ© | `pip install --upgrade pip` |
| `No space left on device` | CUDA torch (~2GB) táº£i Ä‘áº§y disk | `requirements.txt` Ä‘Ã£ dÃ¹ng CPU-only torch |
| `Cannot connect to Ollama` | Ollama chÆ°a cháº¡y | `ollama serve` hoáº·c kiá»ƒm tra port 11434 |
| Pháº£n há»“i cháº­m (>2 phÃºt) | Model quÃ¡ lá»›n cho CPU | Äá»•i sang `qwen2.5:1.5b` trong `.env` |
| OOM / Server crash | RAM khÃ´ng Ä‘á»§ | Äá»•i model nhá» hÆ¡n hoáº·c táº¯t bá»›t app |
| CÃ¢u tráº£ lá»i láº«n lá»™n | Model nhá», context nhiá»…u | Há»i cá»¥ thá»ƒ hÆ¡n, thÃªm tá»« khÃ³a thá»i gian |
