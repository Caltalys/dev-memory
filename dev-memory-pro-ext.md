D∆∞·ªõi ƒë√¢y l√† c√°c file code c√≤n thi·∫øu v√† ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a cho phi√™n b·∫£n **DevMemory Pro**. C√°c file n√†y ƒë∆∞·ª£c vi·∫øt theo chu·∫©n production, x·ª≠ l√Ω l·ªói t·ªët h∆°n, c√≥ logging ƒë·∫ßy ƒë·ªß v√† t√≠ch h·ª£p ch·∫∑t ch·∫Ω v·ªõi ki·∫øn tr√∫c ƒë√£ thi·∫øt k·∫ø.

---

## 1. `app/llm.py` ‚Äî LLM Layer v·ªõi Retry & Streaming

File n√†y x·ª≠ l√Ω vi·ªác giao ti·∫øp v·ªõi Ollama, bao g·ªìm c∆° ch·∫ø retry khi model b·∫≠n v√† format prompt t·ªëi ∆∞u cho RAG.

```python
# app/llm.py
import httpx
import json
from typing import List, Dict, Optional
from app.config import settings, logger

class LLMClient:
    def __init__(self):
        self.base_url = settings.LLM_BASE_URL.rstrip("/")
        self.model = settings.LLM_MODEL
        self.timeout = 120.0  # TƒÉng timeout cho c√¢u h·ªèi ph·ª©c t·∫°p
        self.max_retries = 2
        
        self.system_prompt = """B·∫°n l√† tr·ª£ l√Ω c√° nh√¢n c·ªßa m·ªôt Software Architect/Developer.
B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o Knowledge Base c√° nh√¢n ch·ª©a c√°c ghi ch√∫ k·ªπ thu·∫≠t, quy·∫øt ƒë·ªãnh ki·∫øn tr√∫c v√† b√†i h·ªçc kinh nghi·ªám.

QUY T·∫ÆC TR·∫¢ L·ªúI:
1. Ch·ªâ d·ª±a v√†o context ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin.
2. Tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng (t√™n file, ng√†y th√°ng) trong c√¢u tr·∫£ l·ªùi.
3. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong context, h√£y n√≥i th·∫≥ng "Kh√¥ng c√≥ th√¥ng tin trong knowledge base".
4. ∆Øu ti√™n hi·ªÉn th·ªã code snippet n·∫øu c√≥.
5. Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m k·ªπ thu·∫≠t.
6. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn k·ªπ thu·∫≠t, v·∫´n tr·∫£ l·ªùi l·ªãch s·ª± nh∆∞ng nh·∫Øc nh·ªü v·ªÅ m·ª•c ƒë√≠ch c·ªßa h·ªá th·ªëng.

FORMAT:
- S·ª≠ d·ª•ng Markdown ƒë·ªÉ format code v√† ti√™u ƒë·ªÅ.
- Li·ªát k√™ ngu·ªìn tham kh·∫£o ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi."""

    def _build_prompt(self, query: str, chunks: List[Dict], history: str = "") -> str:
        """X√¢y d·ª±ng prompt v·ªõi context v√† l·ªãch s·ª≠ chat"""
        context_text = ""
        for i, chunk in enumerate(chunks, 1):
            source = chunk.get("metadata", {}).get("filename", "unknown")
            date = chunk.get("metadata", {}).get("date", "")
            tags = chunk.get("metadata", {}).get("tags", "")
            
            context_text += f"""
---
[Source {i}] File: {source} | Date: {date} | Tags: {tags}
Content:
{chunk['content']}
"""
        
        history_section = f"""
L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{history}
""" if history else ""

        prompt = f"""
{context_text}

{history_section}

C√¢u h·ªèi hi·ªán t·∫°i: {query}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n context v√† l·ªãch s·ª≠ tr√™n:"""
        
        return prompt

    def ask(self, query: str, chunks: List[Dict], history: str = "") -> str:
        """G·ª≠i c√¢u h·ªèi ƒë·∫øn Ollama v√† nh·∫≠n c√¢u tr·∫£ l·ªùi"""
        prompt = self._build_prompt(query, chunks, history)
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "system": self.system_prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,  # Th·∫•p ƒë·ªÉ gi·∫£m hallucination
                "top_p": 0.9,
                "num_ctx": 4096,     # Context window l·ªõn h∆°n cho nhi·ªÅu chunks
                "repeat_penalty": 1.1
            }
        }
        
        for attempt in range(self.max_retries + 1):
            try:
                response = httpx.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    timeout=self.timeout
                )
                response.raise_for_status()
                result = response.json()
                return result.get("response", "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi t·ª´ model.")
                
            except httpx.TimeoutException:
                logger.warning(f"Timeout l·∫ßn {attempt + 1}/{self.max_retries + 1}")
                if attempt == self.max_retries:
                    return "‚ö†Ô∏è Timeout: Model ph·∫£n h·ªìi qu√° ch·∫≠m. H√£y th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi ng·∫Øn h∆°n."
                    
            except httpx.ConnectError:
                logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi Ollama. ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y.")
                return "‚ö†Ô∏è L·ªói k·∫øt n·ªëi: Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi Ollama. ƒê·∫£m b·∫£o service ƒëang ch·∫°y."
                
            except Exception as e:
                logger.error(f"L·ªói LLM: {str(e)}")
                if attempt == self.max_retries:
                    return f"‚ö†Ô∏è L·ªói: {str(e)}"
        
        return "‚ö†Ô∏è Kh√¥ng th·ªÉ nh·∫≠n c√¢u tr·∫£ l·ªùi."

    def check_health(self) -> bool:
        """Ki·ªÉm tra k·∫øt n·ªëi ƒë·∫øn Ollama"""
        try:
            response = httpx.get(f"{self.base_url}/api/tags", timeout=5.0)
            return response.status_code == 200
        except:
            return False

# Singleton instance
llm_client = LLMClient()
```

---

## 2. `app/memory.py` ‚Äî Qu·∫£n l√Ω L·ªãch s·ª≠ Chat (SQLite)

File n√†y qu·∫£n l√Ω b·ªô nh·ªõ h·ªôi tho·∫°i, cho ph√©p h·ªèi follow-up v√† l∆∞u l·∫°i l·ªãch s·ª≠ ƒë·ªÉ tra c·ª©u sau n√†y.

```python
# app/memory.py
import sqlite3
from datetime import datetime
from typing import List, Dict, Optional
from contextlib import contextmanager
from app.config import settings, logger

class ChatMemory:
    """Qu·∫£n l√Ω l·ªãch s·ª≠ chat v·ªõi SQLite"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or settings.DB_PATH
        self._init_db()
        logger.info(f"Chat memory initialized: {self.db_path}")
    
    @contextmanager
    def get_connection(self):
        """Context manager cho DB connection"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            logger.error(f"DB Error: {e}")
            raise
        finally:
            conn.close()
    
    def _init_db(self):
        """T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i"""
        with self.get_connection() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    query TEXT NOT NULL,
                    answer TEXT NOT NULL,
                    sources TEXT,
                    chunks_count INTEGER,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT UNIQUE NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    last_active DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_session ON conversations(session_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_created ON conversations(created_at)")
    
    def save_conversation(self, session_id: str, query: str, answer: str, 
                         sources: List[str], chunks_count: int) -> int:
        """L∆∞u m·ªôt l∆∞·ª£t h·ªôi tho·∫°i"""
        with self.get_connection() as conn:
            # Update session last_active
            conn.execute("""
                INSERT OR IGNORE INTO sessions (session_id) VALUES (?)
            """, (session_id,))
            conn.execute("""
                UPDATE sessions SET last_active = CURRENT_TIMESTAMP 
                WHERE session_id = ?
            """, (session_id,))
            
            # Insert conversation
            cursor = conn.execute("""
                INSERT INTO conversations 
                (session_id, query, answer, sources, chunks_count)
                VALUES (?, ?, ?, ?, ?)
            """, (session_id, query, answer, ",".join(sources), chunks_count))
            
            logger.info(f"Saved conversation for session {session_id}")
            return cursor.lastrowid
    
    def get_session_history(self, session_id: str, limit: int = 10) -> List[Dict]:
        """L·∫•y l·ªãch s·ª≠ chat c·ªßa session (cho follow-up questions)"""
        with self.get_connection() as conn:
            cursor = conn.execute("""
                SELECT query, answer, created_at 
                FROM conversations 
                WHERE session_id = ? 
                ORDER BY created_at DESC 
                LIMIT ?
            """, (session_id, limit))
            
            rows = cursor.fetchall()
            # Return reversed (oldest first) for context building
            return [dict(row) for row in reversed(rows)]
    
    def get_recent_history_for_context(self, session_id: str, last_n: int = 3) -> str:
        """X√¢y d·ª±ng chu·ªói l·ªãch s·ª≠ ng·∫Øn cho LLM context"""
        history = self.get_session_history(session_id, limit=last_n)
        if not history:
            return ""
        
        context_lines = []
        for h in history:
            context_lines.append(f"User: {h['query']}")
            context_lines.append(f"Assistant: {h['answer'][:500]}...")  # Gi·ªõi h·∫°n ƒë·ªô d√†i
        return "\n".join(context_lines)
    
    def delete_session(self, session_id: str) -> bool:
        """X√≥a to√†n b·ªô session"""
        with self.get_connection() as conn:
            conn.execute("DELETE FROM conversations WHERE session_id = ?", (session_id,))
            conn.execute("DELETE FROM sessions WHERE session_id = ?", (session_id,))
            logger.info(f"Deleted session {session_id}")
            return True
    
    def get_stats(self) -> Dict:
        """Th·ªëng k√™ s·ª≠ d·ª•ng"""
        with self.get_connection() as conn:
            total_convos = conn.execute(
                "SELECT COUNT(*) FROM conversations"
            ).fetchone()[0]
            total_sessions = conn.execute(
                "SELECT COUNT(*) FROM sessions"
            ).fetchone()[0]
            
            return {
                "total_conversations": total_convos,
                "total_sessions": total_sessions,
                "db_path": self.db_path
            }

# Singleton instance
chat_memory = ChatMemory()
```

---

## 3. `app/watcher.py` ‚Äî Auto-Index v·ªõi Watchdog

File n√†y theo d√µi th∆∞ m·ª•c notes v√† t·ª± ƒë·ªông index khi c√≥ thay ƒë·ªïi, ƒë·∫£m b·∫£o knowledge base lu√¥n c·∫≠p nh·∫≠t.

```python
# app/watcher.py
import time
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent
from typing import Set
from app.config import settings, logger
from app.indexer import Indexer

class NoteFileHandler(FileSystemEventHandler):
    """X·ª≠ l√Ω s·ª± ki·ªán thay ƒë·ªïi file trong th∆∞ m·ª•c notes"""
    
    def __init__(self, indexer: Indexer):
        super().__init__()
        self.indexer = indexer
        self._debounce_set: Set[str] = set()
        self._debounce_delay = 2.0  # Gi√¢y ch·ªù ƒë·ªÉ tr√°nh index nhi·ªÅu l·∫ßn khi save
    
    def _should_process(self, path: str) -> bool:
        """Ki·ªÉm tra file c√≥ n√™n ƒë∆∞·ª£c index kh√¥ng"""
        path_obj = Path(path)
        
        # Ch·ªâ x·ª≠ l√Ω file .md
        if path_obj.suffix.lower() != '.md':
            return False
        
        # B·ªè qua template
        if path_obj.name.lower() == 'template.md':
            return False
        
        # B·ªè qua th∆∞ m·ª•c ·∫©n
        if any(part.startswith('.') for part in path_obj.parts):
            return False
        
        return True
    
    def _debounced_index(self, filepath: Path):
        """Index v·ªõi debounce ƒë·ªÉ tr√°nh trigger nhi·ªÅu l·∫ßn"""
        path_str = str(filepath)
        
        if path_str in self._debounce_set:
            return
        
        self._debounce_set.add(path_str)
        
        # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ file ƒë∆∞·ª£c write ho√†n t·∫•t
        time.sleep(self._debounce_delay)
        
        try:
            if filepath.exists():
                logger.info(f"üîÑ File changed: {filepath.name}")
                self.indexer.index_file(filepath)
            else:
                logger.info(f"üóëÔ∏è File deleted: {filepath.name}")
                # C√≥ th·ªÉ implement logic x√≥a kh·ªèi vector DB ·ªü ƒë√¢y
        except Exception as e:
            logger.error(f"Failed to index {filepath}: {e}")
        finally:
            self._debounce_set.discard(path_str)
    
    def on_modified(self, event):
        """X·ª≠ l√Ω s·ª± ki·ªán file b·ªã s·ª≠a"""
        if isinstance(event, FileModifiedEvent) and self._should_process(event.src_path):
            filepath = Path(event.src_path)
            self._debounced_index(filepath)
    
    def on_created(self, event):
        """X·ª≠ l√Ω s·ª± ki·ªán file m·ªõi ƒë∆∞·ª£c t·∫°o"""
        if isinstance(event, FileCreatedEvent) and self._should_process(event.src_path):
            filepath = Path(event.src_path)
            logger.info(f"üìÑ New file detected: {filepath.name}")
            self._debounced_index(filepath)
    
    def on_deleted(self, event):
        """X·ª≠ l√Ω s·ª± ki·ªán file b·ªã x√≥a"""
        if self._should_process(event.src_path):
            logger.info(f"üóëÔ∏è File deleted: {Path(event.src_path).name}")
            # C√≥ th·ªÉ th√™m logic x√≥a kh·ªèi vector DB

class FileWatcher:
    """Wrapper ƒë·ªÉ qu·∫£n l√Ω Observer"""
    
    def __init__(self, notes_dir: Path = None):
        self.notes_dir = notes_dir or settings.NOTES_DIR
        self.indexer = Indexer()
        self.observer = Observer()
        self.handler = NoteFileHandler(self.indexer)
        self._running = False
    
    def start(self):
        """B·∫Øt ƒë·∫ßu watching"""
        self.observer.schedule(
            self.handler, 
            str(self.notes_dir), 
            recursive=True
        )
        self.observer.start()
        self._running = True
        logger.info(f"üëÄ Watching {self.notes_dir} for changes...")
        logger.info(f"üìä Current index size: {self.indexer.collection.count()} chunks")
    
    def stop(self):
        """D·ª´ng watching"""
        self._running = False
        self.observer.stop()
        self.observer.join()
        logger.info("üõë File watcher stopped")
    
    def run_forever(self):
        """Ch·∫°y watcher v√¥ h·∫°n (cho production)"""
        self.start()
        try:
            while self._running:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("Received interrupt signal")
            self.stop()
    
    def health_check(self) -> bool:
        """Ki·ªÉm tra watcher c√≥ ƒëang ch·∫°y kh√¥ng"""
        return self._running and self.observer.is_alive()

# Singleton cho watcher
file_watcher = FileWatcher()

# Entry point cho running ƒë·ªôc l·∫≠p
if __name__ == "__main__":
    logger.info("üöÄ Starting DevMemory File Watcher...")
    file_watcher.run_forever()
```

---

## 4. `app/main.py` ‚Äî C·∫≠p nh·∫≠t API Server ho√†n ch·ªânh

File n√†y t√≠ch h·ª£p t·∫•t c·∫£ c√°c module l·∫°i v·ªõi nhau th√†nh m·ªôt API ho√†n ch·ªânh.

```python
# app/main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from pydantic import BaseModel
from typing import List, Optional
import uuid
from pathlib import Path

from app.config import settings, logger
from app.indexer import Indexer
from app.retriever import Retriever
from app.llm import llm_client
from app.memory import chat_memory
from app.watcher import file_watcher

# Init FastAPI
app = FastAPI(
    title="DevMemory Pro",
    description="Personal Knowledge Base v·ªõi RAG Local",
    version="2.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Init components
indexer = Indexer()
retriever = Retriever(indexer)

# ==================== Models ====================

class QueryRequest(BaseModel):
    question: str
    session_id: Optional[str] = None
    top_k: int = 5

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    chunks_found: int
    session_id: str
    processing_time_ms: float

class ReindexRequest(BaseModel):
    full_reindex: bool = False

class HealthResponse(BaseModel):
    status: str
    ollama_connected: bool
    total_chunks: int
    total_conversations: int

# ==================== Endpoints ====================

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve UI"""
    ui_path = Path("ui/index.html")
    if ui_path.exists():
        return FileResponse(str(ui_path))
    raise HTTPException(status_code=404, detail="UI not found")

@app.post("/ask", response_model=QueryResponse)
async def ask_question(req: QueryRequest):
    """X·ª≠ l√Ω c√¢u h·ªèi v·ªõi RAG"""
    import time
    start_time = time.time()
    
    # Generate session ID n·∫øu ch∆∞a c√≥
    session_id = req.session_id or str(uuid.uuid4())
    
    # 1. Retrieve
    chunks = retriever.retrieve(req.question, top_k=req.top_k)
    
    # 2. Get history for context
    history_context = chat_memory.get_recent_history_for_context(session_id, last_n=3)
    
    # 3. Generate
    answer = llm_client.ask(req.question, chunks, history_context)
    
    # 4. Save to memory
    sources = list({c.get("metadata", {}).get("filename", "unknown") for c in chunks})
    chat_memory.save_conversation(
        session_id=session_id,
        query=req.question,
        answer=answer,
        sources=sources,
        chunks_count=len(chunks)
    )
    
    processing_time = (time.time() - start_time) * 1000
    
    return QueryResponse(
        answer=answer,
        sources=sources,
        chunks_found=len(chunks),
        session_id=session_id,
        processing_time_ms=round(processing_time, 2)
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Ki·ªÉm tra s·ª©c kh·ªèe h·ªá th·ªëng"""
    ollama_ok = llm_client.check_health()
    stats = chat_memory.get_stats()
    
    return HealthResponse(
        status="healthy" if ollama_ok else "degraded",
        ollama_connected=ollama_ok,
        total_chunks=indexer.collection.count(),
        total_conversations=stats["total_conversations"]
    )

@app.post("/reindex")
async def reindex(background_tasks: BackgroundTasks, req: ReindexRequest = None):
    """Trigger re-index (c√≥ th·ªÉ ch·∫°y background)"""
    full = req.full_reindex if req else False
    
    if full:
        background_tasks.add_task(indexer.index_all)
        return {"status": "started", "message": "Full reindex started in background"}
    else:
        # Ch·ªâ index l·∫°i files hi·ªán t·∫°i (watcher ƒë√£ l√†m vi·ªác n√†y t·ª± ƒë·ªông)
        indexer.index_all()
        return {"status": "completed", "message": f"Indexed {indexer.collection.count()} chunks"}

@app.get("/history/{session_id}")
async def get_history(session_id: str, limit: int = 20):
    """L·∫•y l·ªãch s·ª≠ chat c·ªßa session"""
    history = chat_memory.get_session_history(session_id, limit=limit)
    return {"session_id": session_id, "conversations": history}

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """X√≥a session"""
    success = chat_memory.delete_session(session_id)
    return {"deleted": success, "session_id": session_id}

@app.get("/stats")
async def get_stats():
    """Th·ªëng k√™ h·ªá th·ªëng"""
    return {
        "chat_memory": chat_memory.get_stats(),
        "vector_db": {
            "total_chunks": indexer.collection.count(),
            "collection_name": indexer.collection.name
        },
        "watcher": {
            "running": file_watcher.health_check()
        }
    }

# Mount static files
ui_path = Path("ui")
if ui_path.exists():
    app.mount("/static", StaticFiles(directory=str(ui_path)), name="static")

# Startup event
@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ DevMemory Pro starting...")
    logger.info(f"üìÅ Notes dir: {settings.NOTES_DIR}")
    logger.info(f"üß† LLM Model: {settings.LLM_MODEL}")
    logger.info(f"üîó Embedding: {settings.EMBEDDING_MODEL}")
    
    # Check Ollama connection
    if not llm_client.check_health():
        logger.warning("‚ö†Ô∏è Ollama not connected. Some features may not work.")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app.main:app", host="0.0.0.0", port=8000, reload=True)
```

---

## 5. H∆∞·ªõng d·∫´n ch·∫°y ho√†n ch·ªânh

```bash
# 1. C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt

# 2. ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y
ollama serve  # Terminal 1
ollama pull qwen2.5:3b  # N·∫øu ch∆∞a c√≥

# 3. Index l·∫ßn ƒë·∫ßu
python -m app.indexer  # Ho·∫∑c: python -c "from app.indexer import Indexer; Indexer().index_all()"

# 4. Ch·∫°y Auto-watcher (Optional - n·∫øu mu·ªën auto-index)
python -m app.watcher  # Terminal 2

# 5. Ch·∫°y API Server
python -m app.main  # Terminal 3
# Ho·∫∑c: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# 6. M·ªü browser
http://localhost:8000
```

---

## 6. Ki·ªÉm tra nhanh API

```bash
# Health check
curl http://localhost:8000/health

# H·ªèi c√¢u ƒë·∫ßu ti√™n
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "C√°ch x·ª≠ l√Ω l·ªói N+1 trong Hibernate?"}'

# L·∫•y l·ªãch s·ª≠ session
curl http://localhost:8000/history/{session_id_from_response}

# Stats
curl http://localhost:8000/stats
```

---

V·ªõi c√°c file b·ªï sung n√†y, h·ªá th·ªëng **DevMemory Pro** c·ªßa b·∫°n ƒë√£ ho√†n ch·ªânh v√† s·∫µn s√†ng cho vi·ªác s·ª≠ d·ª•ng th·ª±c t·∫ø h√†ng ng√†y. C√°c t√≠nh nƒÉng ch√≠nh ƒë√£ ƒë∆∞·ª£c cover:

| T√≠nh nƒÉng | File | Tr·∫°ng th√°i |
|-----------|------|------------|
| Index th√¥ng minh | `indexer.py` | ‚úÖ |
| Hybrid Search | `retriever.py` | ‚úÖ |
| LLM v·ªõi Retry | `llm.py` | ‚úÖ |
| Chat Memory | `memory.py` | ‚úÖ |
| Auto-watch | `watcher.py` | ‚úÖ |
| API ho√†n ch·ªânh | `main.py` | ‚úÖ |
| UI | `ui/index.html` | ‚úÖ |