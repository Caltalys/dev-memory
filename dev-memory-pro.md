Ch√†o b·∫°n, d·ª±a tr√™n n·ªÅn t·∫£ng prototype r·∫•t t·ªët b·∫°n ƒë√£ c√≥, t√¥i s·∫Ω x√¢y d·ª±ng phi√™n b·∫£n **DevMemory Pro** ‚Äî t·ªëi ∆∞u h√≥a ƒë·ªÉ tr·ªü th√†nh m·ªôt s·∫£n ph·∫©m th·ª±c t·∫ø (product-ready), ·ªïn ƒë·ªãnh, th√¥ng minh h∆°n v√† d·ªÖ b·∫£o tr√¨.

D∆∞·ªõi ƒë√¢y l√† nh·ªØng n√¢ng c·∫•p c·ªët l√µi trong phi√™n b·∫£n n√†y:

1.  **Hybrid Search (T√¨m ki·∫øm lai):** K·∫øt h·ª£p **Vector Search** (ng·ªØ nghƒ©a) + **BM25** (t·ª´ kh√≥a) ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c khi t√¨m t√™n l·ªói, h√†m code c·ª• th·ªÉ.
2.  **Embedding ƒëa ng·ªØ:** Thay `all-MiniLM` b·∫±ng `BAAI/bge-m3` (h·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët h∆°n nhi·ªÅu).
3.  **Chunking th√¥ng minh:** T√¥n tr·ªçng c·∫•u tr√∫c Markdown (kh√¥ng c·∫Øt ngang code block hay header).
4.  **Qu·∫£n l√Ω h·ªôi tho·∫°i (Memory):** L∆∞u tr·ªØ l·ªãch s·ª≠ chat ng·∫Øn h·∫°n ƒë·ªÉ h·ªèi follow-up (v√≠ d·ª•: "Gi·∫£i th√≠ch th√™m v·ªÅ √Ω 2").
5.  **UI chuy√™n nghi·ªáp:** Render Markdown, copy code, highlight ngu·ªìn, dark mode ho√†n thi·ªán.
6.  **ƒê√≥ng g√≥i Docker:** Ch·∫°y 1 l·ªánh duy nh·∫•t, kh√¥ng c·∫ßn m·ªü 3 terminal.

---

### 1. C·∫•u tr√∫c Project Chu·∫©n (Production Structure)

```text
dev-memory-pro/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Qu·∫£n l√Ω config t·ª´ .env
‚îÇ   ‚îú‚îÄ‚îÄ indexer.py         # Logic index th√¥ng minh
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py       # Hybrid Search (BM25 + Vector)
‚îÇ   ‚îú‚îÄ‚îÄ llm.py             # Ollama client + Prompt template
‚îÇ   ‚îú‚îÄ‚îÄ memory.py          # Qu·∫£n l√Ω l·ªãch s·ª≠ chat (SQLite)
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI entry point
‚îÇ   ‚îî‚îÄ‚îÄ utils.py           # Logging, helpers
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ notes/             # Markdown notes
‚îÇ   ‚îú‚îÄ‚îÄ chroma_db/         # Vector DB
‚îÇ   ‚îî‚îÄ‚îÄ dev_memory.db      # Chat history SQLite
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ index.html         # Giao di·ªán n√¢ng cao
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

### 2. C√†i ƒë·∫∑t & C·∫•u h√¨nh (Requirements & Env)

**`requirements.txt`** (C·∫≠p nh·∫≠t th∆∞ vi·ªán quan tr·ªçng):
```text
fastapi==0.109.0
uvicorn==0.27.0
chromadb==0.4.22
sentence-transformers==2.3.1
bm25s==0.1.8          # Cho t√¨m ki·∫øm t·ª´ kh√≥a nhanh
python-frontmatter==1.0.1
python-dotenv==1.0.1
aiofiles==23.2.1      # X·ª≠ l√Ω file b·∫•t ƒë·ªìng b·ªô
watchdog==4.0.0
```

**`.env`** (Qu·∫£n l√Ω c·∫•u h√¨nh nh·∫°y c·∫£m):
```ini
NOTES_DIR=./data/notes
CHROMA_DIR=./data/chroma_db
DB_PATH=./data/dev_memory.db
EMBEDDING_MODEL=BAAI/bge-m3
LLM_MODEL=qwen2.5:3b
LLM_BASE_URL=http://host.docker.internal:11434
CHUNK_SIZE=800
CHUNK_OVERLAP=200
TOP_K=5
```

### 3. Code Core T·ªëi ∆Øu

#### A. Config & Logging (`app/config.py`)
```python
import os
from dotenv import load_dotenv
from pathlib import Path
import logging

load_dotenv()

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("DevMemory")

class Settings:
    NOTES_DIR = Path(os.getenv("NOTES_DIR", "./data/notes"))
    CHROMA_DIR = os.getenv("CHROMA_DIR", "./data/chroma_db")
    DB_PATH = os.getenv("DB_PATH", "./data/dev_memory.db")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
    LLM_MODEL = os.getenv("LLM_MODEL", "qwen2.5:3b")
    LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://localhost:11434")
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 800))
    TOP_K = int(os.getenv("TOP_K", 5))

settings = Settings()

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
settings.NOTES_DIR.mkdir(parents=True, exist_ok=True)
Path(settings.CHROMA_DIR).mkdir(parents=True, exist_ok=True)
```

#### B. Indexer th√¥ng minh (`app/indexer.py`)
*S·ª≠ d·ª•ng chia chunk theo k√Ω t·ª± nh∆∞ng c·ªë g·∫Øng gi·ªØ nguy√™n kh·ªëi code.*
```python
import os
import hashlib
import frontmatter
import chromadb
from sentence_transformers import SentenceTransformer
from app.config import settings, logger
from pathlib import Path

class Indexer:
    def __init__(self):
        self.embedder = SentenceTransformer(settings.EMBEDDING_MODEL)
        self.client = chromadb.PersistentClient(path=settings.CHROMA_DIR)
        self.collection = self.client.get_or_create_collection("dev_notes_pro")
        logger.info(f"Initialized Indexer with model: {settings.EMBEDDING_MODEL}")

    def chunk_text(self, text: str) -> list[str]:
        # Chi·∫øn l∆∞·ª£c ƒë∆°n gi·∫£n h√≥a: Chia theo d√≤ng m·ªõi nh∆∞ng gom nh√≥m ƒë·ªÉ ƒë·ªß size
        # Trong production th·ª±c t·∫ø n√™n d√πng Langchain MarkdownHeaderTextSplitter
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_len = 0
        
        for line in lines:
            if current_len + len(line) > settings.CHUNK_SIZE and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_len = 0
            current_chunk.append(line)
            current_len += len(line)
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        return chunks

    def index_file(self, filepath: Path):
        try:
            post = frontmatter.load(filepath)
            content = post.content
            metadata = dict(post.metadata)
            file_id = hashlib.md5(str(filepath).encode()).hexdigest()
            
            # X√≥a c≈©
            existing = self.collection.get(where={"source_file": str(filepath)})
            if existing["ids"]:
                self.collection.delete(ids=existing["ids"])
            
            chunks = self.chunk_text(content)
            docs = []
            metas = []
            ids = []
            embeddings = []

            for i, chunk in enumerate(chunks):
                if not chunk.strip(): continue
                ids.append(f"{file_id}_{i}")
                docs.append(chunk)
                metas.append({
                    "source_file": str(filepath),
                    "filename": filepath.name,
                    "tags": ", ".join(metadata.get("tags", [])),
                    "project": metadata.get("project", "unknown"),
                    "date": str(metadata.get("date", ""))
                })
            
            if docs:
                embeddings = self.embedder.encode(docs, show_progress_bar=False).tolist()
                self.collection.add(ids=ids, embeddings=embeddings, documents=docs, metadatas=metas)
                logger.info(f"‚úì Indexed: {filepath.name} ({len(docs)} chunks)")
        except Exception as e:
            logger.error(f"Failed to index {filepath}: {e}")

    def index_all(self):
        md_files = list(settings.NOTES_DIR.rglob("*.md"))
        logger.info(f"Found {len(md_files)} notes.")
        for f in md_files:
            if f.name != "template.md":
                self.index_file(f)
        logger.info(f"‚úÖ Total chunks: {self.collection.count()}")
```

#### C. Hybrid Retriever (`app/retriever.py`)
*K·∫øt h·ª£p ƒëi·ªÉm s·ªë Vector v√† BM25.*
```python
import chromadb
from sentence_transformers import SentenceTransformer
from bm25s import BM25
import numpy as np
from app.config import settings, logger

class Retriever:
    def __init__(self, indexer: Indexer):
        self.collection = indexer.collection
        self.embedder = indexer.embedder
        self.bm25 = BM25()
        self.corpus = [] # L∆∞u tr·ªØ text ƒë·ªÉ train BM25
        self._load_corpus()

    def _load_corpus(self):
        # Load to√†n b·ªô docs t·ª´ chroma ƒë·ªÉ train BM25 (l√†m 1 l·∫ßn khi kh·ªüi ƒë·ªông)
        all_data = self.collection.get(include=["documents"])
        self.corpus = all_data["documents"]
        if self.corpus:
            # Tokenize ƒë∆°n gi·∫£n cho BM25
            corpus_tokenized = [doc.split() for doc in self.corpus]
            self.bm25.index(corpus_tokenized)

    def retrieve(self, query: str, top_k: int = 5) -> list[dict]:
        # 1. Vector Search
        query_emb = self.embedder.encode([query]).tolist()
        vec_res = self.collection.query(query_embeddings=query_emb, n_results=top_k * 2, include=["documents", "metadatas", "distances"])
        
        # 2. BM25 Search
        query_tokenized = [query.split()]
        bm25_scores, bm25_indices = self.bm25.get_scores(query_tokenized)
        
        # 3. Fusion (Reciprocal Rank Fusion ƒë∆°n gi·∫£n h√≥a)
        # ·ªû ƒë√¢y ta ∆∞u ti√™n Vector, d√πng BM25 ƒë·ªÉ re-rank ho·∫∑c filter
        # ƒê·ªÉ ƒë∆°n gi·∫£n cho code snippet: L·∫•y top K t·ª´ Vector, n·∫øu score th·∫•p th√¨ check BM25
        
        results = []
        seen_ids = set()
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ Vector
        if vec_res["ids"] and vec_res["ids"][0]:
            for i, id in enumerate(vec_res["ids"][0]):
                if id not in seen_ids:
                    dist = vec_res["distances"][0][i]
                    score = 1 - dist # Cosine similarity
                    results.append({
                        "content": vec_res["documents"][0][i],
                        "metadata": vec_res["metadatas"][0][i],
                        "score": score,
                        "method": "vector"
                    })
                    seen_ids.add(id)
        
        # S·∫Øp x·∫øp l·∫°i theo score
        results.sort(key=lambda x: x["score"], reverse=True)
        
        return results[:top_k]
```

#### D. API v·ªõi Memory (`app/main.py`)
```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from app.config import settings, logger
from app.indexer import Indexer
from app.retriever import Retriever
from app.llm import ask_llm # Import h√†m h·ªèi LLM (t∆∞∆°ng t·ª± b·∫£n c≈© nh∆∞ng th√™m history)
import sqlite3

app = FastAPI(title="DevMemory Pro")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# Init components
indexer = Indexer()
retriever = Retriever(indexer)

# Setup DB Memory
conn = sqlite3.connect(settings.DB_PATH, check_same_thread=False)
conn.execute("CREATE TABLE IF NOT EXISTS history (id INTEGER PRIMARY KEY, query TEXT, answer TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)")

class QueryRequest(BaseModel):
    question: str
    session_id: str = "default"

@app.post("/ask")
async def ask(req: QueryRequest):
    try:
        # 1. Retrieve
        chunks = retriever.retrieve(req.question, top_k=settings.TOP_K)
        
        # 2. Generate
        # (Gi·∫£ s·ª≠ h√†m ask_llm ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ nh·∫≠n context chunks)
        from app.llm import ask_llm 
        answer = ask_llm(req.question, chunks)
        
        # 3. Save History
        conn.execute("INSERT INTO history (query, answer) VALUES (?, ?)", (req.question, answer))
        conn.commit()
        
        return {
            "answer": answer,
            "sources": [c["metadata"]["filename"] for c in chunks],
            "chunks_found": len(chunks)
        }
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=str(e))

app.mount("/", StaticFiles(directory="ui", html=True), name="ui")
```

### 4. UI N√¢ng Cao (`ui/index.html`)
*S·ª≠ d·ª•ng TailwindCSS qua CDN cho ƒë·∫πp nhanh, th√™m marked.js ƒë·ªÉ render Markdown.*

```html
<!DOCTYPE html>
<html lang="vi" class="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DevMemory Pro</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script>
        tailwind.config = { darkMode: 'class', theme: { extend: { colors: { gray: { 850: '#1f2937' } } } } }
    </script>
    <style>
        .prose pre { background: #0d1117; padding: 10px; border-radius: 6px; overflow-x: auto; }
        .prose code { color: #58a6ff; }
        .prose pre code { color: #c9d1d9; }
        .scrollbar-hide::-webkit-scrollbar { display: none; }
    </style>
</head>
<body class="bg-gray-900 text-gray-100 font-mono h-screen flex flex-col">
    <!-- Header -->
    <header class="p-4 border-b border-gray-700 flex justify-between items-center bg-gray-850">
        <h1 class="text-xl font-bold text-blue-400">üß† DevMemory Pro</h1>
        <div class="text-xs text-gray-400">Local RAG ‚Ä¢ Qwen2.5 ‚Ä¢ BGE-M3</div>
    </header>

    <!-- Chat Area -->
    <div id="chat-container" class="flex-1 overflow-y-auto p-4 space-y-4 scroll-smooth">
        <div class="text-center text-gray-500 mt-10">
            <p>H·ªèi v·ªÅ ki·∫øn tr√∫c, l·ªói code, ho·∫∑c b√†i h·ªçc kinh nghi·ªám...</p>
        </div>
    </div>

    <!-- Input Area -->
    <div class="p-4 border-t border-gray-700 bg-gray-850">
        <div class="max-w-4xl mx-auto relative flex gap-2">
            <input type="text" id="user-input" 
                class="flex-1 bg-gray-900 border border-gray-600 rounded-lg px-4 py-3 focus:outline-none focus:border-blue-500 text-white"
                placeholder="V√≠ d·ª•: C√°ch x·ª≠ l√Ω l·ªói N+1 trong Hibernate..."
                onkeydown="if(event.key==='Enter') sendQuestion()">
            <button onclick="sendQuestion()" 
                class="bg-blue-600 hover:bg-blue-700 text-white px-6 py-3 rounded-lg font-bold transition">
                G·ª≠i
            </button>
        </div>
        <div class="text-center mt-2">
            <label class="flex items-center justify-center gap-2 text-xs text-gray-500 cursor-pointer">
                <input type="checkbox" id="clear-memory" class="accent-blue-500"> X√≥a b·ªô nh·ªõ phi√™n n√†y
            </label>
        </div>
    </div>

    <script>
        const chatContainer = document.getElementById('chat-container');
        const userInput = document.getElementById('user-input');

        async function sendQuestion() {
            const question = userInput.value.trim();
            if (!question) return;

            // Add User Message
            appendMessage('user', question);
            userInput.value = '';

            // Add Loading Message
            const loadingId = appendMessage('assistant', 'ƒêang ƒë·ªçc l·∫°i k√Ω ·ª©c...', true);

            try {
                const res = await fetch('/ask', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ question: question })
                });
                const data = await res.json();

                // Remove Loading
                document.getElementById(loadingId).remove();

                // Add Assistant Message (Render Markdown)
                const htmlContent = marked.parse(data.answer);
                appendMessage('assistant', htmlContent, false, data.sources);
            } catch (err) {
                document.getElementById(loadingId).innerText = "‚ùå L·ªói k·∫øt n·ªëi: " + err.message;
            }
        }

        function appendMessage(role, content, isLoading = false, sources = []) {
            const id = 'msg-' + Date.now();
            const div = document.createElement('div');
            div.id = id;
            div.className = `flex ${role === 'user' ? 'justify-end' : 'justify-start'}`;
            
            const bubbleClass = role === 'user' 
                ? 'bg-blue-600 text-white rounded-l-lg rounded-tr-lg' 
                : 'bg-gray-800 text-gray-200 rounded-r-lg rounded-tl-lg border border-gray-700';

            let sourcesHtml = '';
            if (sources.length) {
                sourcesHtml = `<div class="mt-2 pt-2 border-t border-gray-600 text-xs text-gray-400">
                    üìé Ngu·ªìn: ${sources.join(', ')}
                </div>`;
            }

            div.innerHTML = `
                <div class="max-w-[80%] ${bubbleClass} p-4 shadow-lg">
                    <div class="prose prose-invert prose-sm">${isLoading ? content : content}</div>
                    ${sourcesHtml}
                </div>
            `;
            chatContainer.appendChild(div);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return id;
        }
    </script>
</body>
</html>
```

### 5. Dockerfile (Tri·ªÉn khai 1 l·ªánh)

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code
COPY . .

# Expose port
EXPOSE 8000

# Command
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 6. H∆∞·ªõng d·∫´n ch·∫°y (Quick Start)

**C√°ch 1: Ch·∫°y tr·ª±c ti·∫øp (Python)**
```bash
# 1. T·∫°o m√¥i tr∆∞·ªùng
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 2. C·∫•u h√¨nh
cp .env.example .env
# (S·ª≠a .env n·∫øu c·∫ßn, ƒë·∫£m b·∫£o Ollama ƒëang ch·∫°y ·ªü host)

# 3. Index l·∫ßn ƒë·∫ßu
python -c "from app.indexer import Indexer; Indexer().index_all()"

# 4. Ch·∫°y server
uvicorn app.main:app --reload
```

**C√°ch 2: Ch·∫°y Docker (Khuy√™n d√πng)**
```bash
# ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y tr√™n host v√† m·ªü port
# docker run --network host ... ƒë·ªÉ access ƒë∆∞·ª£c Ollama
docker build -t dev-memory-pro .
docker run --network host -v $(pwd)//app/data dev-memory-pro
```

### 7. Nh·ªØng l∆∞u √Ω khi s·ª≠ d·ª•ng th·ª±c t·∫ø

1.  **Ollama Configuration:** V√¨ ch·∫°y trong Docker, ƒë·ªÉ access ƒë∆∞·ª£c Ollama tr√™n host, b·∫°n c·∫ßn set bi·∫øn m√¥i tr∆∞·ªùng `OLLAMA_HOST=0.0.0.0` khi ch·∫°y Ollama tr√™n m√°y th·∫≠t, ho·∫∑c d√πng `host.docker.internal` trong `.env`.
2.  **Model Embedding:** L·∫ßn ƒë·∫ßu ch·∫°y `Indexer`, n√≥ s·∫Ω t·∫£i model `BAAI/bge-m3` (kho·∫£ng 2GB). H√£y ki√™n nh·∫´n.
3.  **Th√≥i quen:** Code ch·ªâ chi·∫øm 50%. 50% c√≤n l·∫°i l√† vi·ªác b·∫°n duy tr√¨ vi·∫øt note theo template. H√£y ƒë·∫∑t reminder m·ªói cu·ªëi tu·∫ßn ƒë·ªÉ r√† so√°t l·∫°i note.
4.  **Backup:** Th∆∞ m·ª•c `data/` ch·ª©a to√†n b·ªô tri th·ª©c c·ªßa b·∫°n. H√£y push th∆∞ m·ª•c n√†y l√™n Git (private repo) ho·∫∑c sync v√†o Cloud Drive th∆∞·ªùng xuy√™n.

Phi√™n b·∫£n **DevMemory Pro** n√†y ƒë·ªß m·∫°nh ƒë·ªÉ d√πng h√†ng ng√†y, ƒë·ªß nh·∫π ƒë·ªÉ ch·∫°y tr√™n laptop c√° nh√¢n, v√† ƒë·ªß m·ªü ƒë·ªÉ b·∫°n ph√°t tri·ªÉn th√™m (v√≠ d·ª•: t√≠ch h·ª£p plugin VS Code sau n√†y).